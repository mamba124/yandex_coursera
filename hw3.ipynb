{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1     2     3     4    5     6     7     8     9     10    11    12  \\\n",
       "0   1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04  3.92   \n",
       "1   1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05  3.40   \n",
       "2   1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
       "3   1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
       "4   1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
       "\n",
       "     13  \n",
       "0  1065  \n",
       "1  1050  \n",
       "2  1185  \n",
       "3  1480  \n",
       "4   735  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_d = pd.read_csv('C:\\Tomato\\wine\\wine_data.data', header = None)\n",
    "wine_d.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1     2     3     4    5     6     7     8     9          10    11  \\\n",
      "0    14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29   5.640000  1.04   \n",
      "1    13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28   4.380000  1.05   \n",
      "2    13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81   5.680000  1.03   \n",
      "3    14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18   7.800000  0.86   \n",
      "4    13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82   4.320000  1.04   \n",
      "5    14.20  1.76  2.45  15.2  112  3.27  3.39  0.34  1.97   6.750000  1.05   \n",
      "6    14.39  1.87  2.45  14.6   96  2.50  2.52  0.30  1.98   5.250000  1.02   \n",
      "7    14.06  2.15  2.61  17.6  121  2.60  2.51  0.31  1.25   5.050000  1.06   \n",
      "8    14.83  1.64  2.17  14.0   97  2.80  2.98  0.29  1.98   5.200000  1.08   \n",
      "9    13.86  1.35  2.27  16.0   98  2.98  3.15  0.22  1.85   7.220000  1.01   \n",
      "10   14.10  2.16  2.30  18.0  105  2.95  3.32  0.22  2.38   5.750000  1.25   \n",
      "11   14.12  1.48  2.32  16.8   95  2.20  2.43  0.26  1.57   5.000000  1.17   \n",
      "12   13.75  1.73  2.41  16.0   89  2.60  2.76  0.29  1.81   5.600000  1.15   \n",
      "13   14.75  1.73  2.39  11.4   91  3.10  3.69  0.43  2.81   5.400000  1.25   \n",
      "14   14.38  1.87  2.38  12.0  102  3.30  3.64  0.29  2.96   7.500000  1.20   \n",
      "15   13.63  1.81  2.70  17.2  112  2.85  2.91  0.30  1.46   7.300000  1.28   \n",
      "16   14.30  1.92  2.72  20.0  120  2.80  3.14  0.33  1.97   6.200000  1.07   \n",
      "17   13.83  1.57  2.62  20.0  115  2.95  3.40  0.40  1.72   6.600000  1.13   \n",
      "18   14.19  1.59  2.48  16.5  108  3.30  3.93  0.32  1.86   8.700000  1.23   \n",
      "19   13.64  3.10  2.56  15.2  116  2.70  3.03  0.17  1.66   5.100000  0.96   \n",
      "20   14.06  1.63  2.28  16.0  126  3.00  3.17  0.24  2.10   5.650000  1.09   \n",
      "21   12.93  3.80  2.65  18.6  102  2.41  2.41  0.25  1.98   4.500000  1.03   \n",
      "22   13.71  1.86  2.36  16.6  101  2.61  2.88  0.27  1.69   3.800000  1.11   \n",
      "23   12.85  1.60  2.52  17.8   95  2.48  2.37  0.26  1.46   3.930000  1.09   \n",
      "24   13.50  1.81  2.61  20.0   96  2.53  2.61  0.28  1.66   3.520000  1.12   \n",
      "25   13.05  2.05  3.22  25.0  124  2.63  2.68  0.47  1.92   3.580000  1.13   \n",
      "26   13.39  1.77  2.62  16.1   93  2.85  2.94  0.34  1.45   4.800000  0.92   \n",
      "27   13.30  1.72  2.14  17.0   94  2.40  2.19  0.27  1.35   3.950000  1.02   \n",
      "28   13.87  1.90  2.80  19.4  107  2.95  2.97  0.37  1.76   4.500000  1.25   \n",
      "29   14.02  1.68  2.21  16.0   96  2.65  2.33  0.26  1.98   4.700000  1.04   \n",
      "..     ...   ...   ...   ...  ...   ...   ...   ...   ...        ...   ...   \n",
      "148  13.32  3.24  2.38  21.5   92  1.93  0.76  0.45  1.25   8.420000  0.55   \n",
      "149  13.08  3.90  2.36  21.5  113  1.41  1.39  0.34  1.14   9.400000  0.57   \n",
      "150  13.50  3.12  2.62  24.0  123  1.40  1.57  0.22  1.25   8.600000  0.59   \n",
      "151  12.79  2.67  2.48  22.0  112  1.48  1.36  0.24  1.26  10.800000  0.48   \n",
      "152  13.11  1.90  2.75  25.5  116  2.20  1.28  0.26  1.56   7.100000  0.61   \n",
      "153  13.23  3.30  2.28  18.5   98  1.80  0.83  0.61  1.87  10.520000  0.56   \n",
      "154  12.58  1.29  2.10  20.0  103  1.48  0.58  0.53  1.40   7.600000  0.58   \n",
      "155  13.17  5.19  2.32  22.0   93  1.74  0.63  0.61  1.55   7.900000  0.60   \n",
      "156  13.84  4.12  2.38  19.5   89  1.80  0.83  0.48  1.56   9.010000  0.57   \n",
      "157  12.45  3.03  2.64  27.0   97  1.90  0.58  0.63  1.14   7.500000  0.67   \n",
      "158  14.34  1.68  2.70  25.0   98  2.80  1.31  0.53  2.70  13.000000  0.57   \n",
      "159  13.48  1.67  2.64  22.5   89  2.60  1.10  0.52  2.29  11.750000  0.57   \n",
      "160  12.36  3.83  2.38  21.0   88  2.30  0.92  0.50  1.04   7.650000  0.56   \n",
      "161  13.69  3.26  2.54  20.0  107  1.83  0.56  0.50  0.80   5.880000  0.96   \n",
      "162  12.85  3.27  2.58  22.0  106  1.65  0.60  0.60  0.96   5.580000  0.87   \n",
      "163  12.96  3.45  2.35  18.5  106  1.39  0.70  0.40  0.94   5.280000  0.68   \n",
      "164  13.78  2.76  2.30  22.0   90  1.35  0.68  0.41  1.03   9.580000  0.70   \n",
      "165  13.73  4.36  2.26  22.5   88  1.28  0.47  0.52  1.15   6.620000  0.78   \n",
      "166  13.45  3.70  2.60  23.0  111  1.70  0.92  0.43  1.46  10.680000  0.85   \n",
      "167  12.82  3.37  2.30  19.5   88  1.48  0.66  0.40  0.97  10.260000  0.72   \n",
      "168  13.58  2.58  2.69  24.5  105  1.55  0.84  0.39  1.54   8.660000  0.74   \n",
      "169  13.40  4.60  2.86  25.0  112  1.98  0.96  0.27  1.11   8.500000  0.67   \n",
      "170  12.20  3.03  2.32  19.0   96  1.25  0.49  0.40  0.73   5.500000  0.66   \n",
      "171  12.77  2.39  2.28  19.5   86  1.39  0.51  0.48  0.64   9.899999  0.57   \n",
      "172  14.16  2.51  2.48  20.0   91  1.68  0.70  0.44  1.24   9.700000  0.62   \n",
      "173  13.71  5.65  2.45  20.5   95  1.68  0.61  0.52  1.06   7.700000  0.64   \n",
      "174  13.40  3.91  2.48  23.0  102  1.80  0.75  0.43  1.41   7.300000  0.70   \n",
      "175  13.27  4.28  2.26  20.0  120  1.59  0.69  0.43  1.35  10.200000  0.59   \n",
      "176  13.17  2.59  2.37  20.0  120  1.65  0.68  0.53  1.46   9.300000  0.60   \n",
      "177  14.13  4.10  2.74  24.5   96  2.05  0.76  0.56  1.35   9.200000  0.61   \n",
      "\n",
      "       12  \n",
      "0    3.92  \n",
      "1    3.40  \n",
      "2    3.17  \n",
      "3    3.45  \n",
      "4    2.93  \n",
      "5    2.85  \n",
      "6    3.58  \n",
      "7    3.58  \n",
      "8    2.85  \n",
      "9    3.55  \n",
      "10   3.17  \n",
      "11   2.82  \n",
      "12   2.90  \n",
      "13   2.73  \n",
      "14   3.00  \n",
      "15   2.88  \n",
      "16   2.65  \n",
      "17   2.57  \n",
      "18   2.82  \n",
      "19   3.36  \n",
      "20   3.71  \n",
      "21   3.52  \n",
      "22   4.00  \n",
      "23   3.63  \n",
      "24   3.82  \n",
      "25   3.20  \n",
      "26   3.22  \n",
      "27   2.77  \n",
      "28   3.40  \n",
      "29   3.59  \n",
      "..    ...  \n",
      "148  1.62  \n",
      "149  1.33  \n",
      "150  1.30  \n",
      "151  1.47  \n",
      "152  1.33  \n",
      "153  1.51  \n",
      "154  1.55  \n",
      "155  1.48  \n",
      "156  1.64  \n",
      "157  1.73  \n",
      "158  1.96  \n",
      "159  1.78  \n",
      "160  1.58  \n",
      "161  1.82  \n",
      "162  2.11  \n",
      "163  1.75  \n",
      "164  1.68  \n",
      "165  1.75  \n",
      "166  1.56  \n",
      "167  1.75  \n",
      "168  1.80  \n",
      "169  1.92  \n",
      "170  1.83  \n",
      "171  1.63  \n",
      "172  1.71  \n",
      "173  1.74  \n",
      "174  1.56  \n",
      "175  1.56  \n",
      "176  1.62  \n",
      "177  1.60  \n",
      "\n",
      "[178 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "X = wine_d.iloc[:,1:13]\n",
    "y = wine_d.iloc[:,0]\n",
    "print(wine_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [  0   1   2   3   4   5   6   7   8  10  11  13  14  17  20  21  22  23\n",
      "  25  26  27  28  32  33  34  35  36  37  38  39  40  43  44  46  47  48\n",
      "  49  50  51  52  53  54  56  57  58  59  61  62  63  64  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  83  84  85  86  87  88  89  91\n",
      "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 110\n",
      " 112 115 116 120 121 122 123 124 125 126 127 129 130 131 132 133 134 135\n",
      " 136 137 138 139 142 143 144 146 147 148 149 151 152 153 154 155 156 157\n",
      " 158 159 160 161 162 163 165 166 167 168 170 172 173 175 176 177] | Test: [  9  12  15  16  18  19  24  29  30  31  41  42  45  55  60  65  66  67\n",
      "  82  90 109 111 113 114 117 118 119 128 140 141 145 150 164 169 171 174]\n",
      "Train: [  0   1   3   5   7   8   9  10  12  13  14  15  16  17  18  19  20  21\n",
      "  23  24  25  28  29  30  31  33  34  35  37  39  40  41  42  43  44  45\n",
      "  46  47  48  49  50  52  53  54  55  57  58  59  60  61  62  63  64  65\n",
      "  66  67  70  71  72  73  74  75  77  79  80  81  82  83  84  86  87  88\n",
      "  89  90  91  92  94  96  99 101 102 103 105 106 107 109 110 111 112 113\n",
      " 114 115 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132\n",
      " 133 134 135 136 139 140 141 142 145 147 148 149 150 151 152 155 156 157\n",
      " 160 161 162 163 164 165 166 168 169 171 172 173 174 175 176 177] | Test: [  2   4   6  11  22  26  27  32  36  38  51  56  68  69  76  78  85  93\n",
      "  95  97  98 100 104 108 122 137 138 143 144 146 153 154 158 159 167 170]\n",
      "Train: [  1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  17  18  19\n",
      "  20  21  22  24  26  27  29  30  31  32  34  35  36  37  38  41  42  43\n",
      "  45  46  48  49  50  51  52  53  54  55  56  57  58  59  60  63  65  66\n",
      "  67  68  69  70  71  72  74  76  77  78  80  82  83  85  87  88  89  90\n",
      "  91  92  93  95  97  98  99 100 102 103 104 105 106 107 108 109 110 111\n",
      " 112 113 114 115 116 117 118 119 121 122 124 128 129 130 131 134 136 137\n",
      " 138 140 141 143 144 145 146 148 149 150 151 152 153 154 155 156 157 158\n",
      " 159 160 161 163 164 165 167 168 169 170 171 172 174 175 176 177] | Test: [  0  10  23  25  28  33  39  40  44  47  61  62  64  73  75  79  81  84\n",
      "  86  94  96 101 120 123 125 126 127 132 133 135 139 142 147 162 166 173]\n",
      "Train: [  0   1   2   4   6   9  10  11  12  14  15  16  18  19  20  21  22  23\n",
      "  24  25  26  27  28  29  30  31  32  33  36  37  38  39  40  41  42  44\n",
      "  45  47  48  50  51  52  54  55  56  57  58  60  61  62  63  64  65  66\n",
      "  67  68  69  71  73  74  75  76  78  79  81  82  84  85  86  87  88  90\n",
      "  92  93  94  95  96  97  98  99 100 101 102 103 104 106 107 108 109 111\n",
      " 113 114 116 117 118 119 120 121 122 123 125 126 127 128 129 130 132 133\n",
      " 135 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 153 154\n",
      " 157 158 159 162 163 164 166 167 168 169 170 171 172 173 174 175 177] | Test: [  3   5   7   8  13  17  34  35  43  46  49  53  59  70  72  77  80  83\n",
      "  89  91 105 110 112 115 124 131 134 136 152 155 156 160 161 165 176]\n",
      "Train: [  0   2   3   4   5   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
      "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  38  39  40\n",
      "  41  42  43  44  45  46  47  49  51  53  55  56  59  60  61  62  64  65\n",
      "  66  67  68  69  70  72  73  75  76  77  78  79  80  81  82  83  84  85\n",
      "  86  89  90  91  93  94  95  96  97  98 100 101 104 105 108 109 110 111\n",
      " 112 113 114 115 117 118 119 120 122 123 124 125 126 127 128 131 132 133\n",
      " 134 135 136 137 138 139 140 141 142 143 144 145 146 147 150 152 153 154\n",
      " 155 156 158 159 160 161 162 164 165 166 167 169 170 171 173 174 176] | Test: [  1  14  20  21  37  48  50  52  54  57  58  63  71  74  87  88  92  99\n",
      " 102 103 106 107 116 121 129 130 148 149 151 157 163 168 172 175 177]\n",
      "0.88,,,,3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "for train_indices, test_indices in cv.split(X):\n",
    "\n",
    "    print('Train: %s | Test: %s' % (train_indices,test_indices))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "res = {}\n",
    "accur_max, k_max = 0,0 \n",
    "for k in range(1, 51):\n",
    "\n",
    "    estimator = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "    res[k] = np.mean(cross_val_score(estimator, X, y, cv=kf, scoring='accuracy'))\n",
    "    if res[k] > accur_max:\n",
    "\n",
    "        accur_max = res[k]\n",
    "\n",
    "        k_max = k\n",
    "        \n",
    "print(round(accur_max,2), k_max, sep = ',,,,')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
